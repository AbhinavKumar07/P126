{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AbhinavKumar07/P126/blob/main/P126_V2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Archery-Target** is a game in which the players shoot sharp-pointed arrows at a round target having 10 rings.\n",
        "\n",
        "<img src=\"https://s3-whjr-curriculum-uploads.whjr.online/4de9132a-c71d-42ce-9099-3293e8805fd9.jpg\">"
      ],
      "metadata": {
        "id": "nUWO5QkC_g-4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RL Problem to Solve\n",
        "Hit the center of the target with maximum reward"
      ],
      "metadata": {
        "id": "5QtHLAqv3wP3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://s3-whjr-curriculum-uploads.whjr.online/40656a8c-14e2-4dd7-9f9e-4c17669b9182.png\" width=300>\n",
        "\n",
        "\n",
        "Number of **State**: ?\n",
        "\n",
        "Number of **Actions**: ?\n",
        "\n"
      ],
      "metadata": {
        "id": "Osb6FQ74YZtE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import required libraries\n",
        "import numpy as np\n",
        "import random"
      ],
      "metadata": {
        "id": "M2oIipDmeqap"
      },
      "execution_count": 190,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reward Matrix\n",
        "Reward Matrix represents states as rows and actions as columns with respective awards values assigned for a given state and action pair."
      ],
      "metadata": {
        "id": "Ujmi3BO54LfJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Create reward matrix\n",
        "\n",
        "reward_matrix = np.array([0,1,2,3,4,5,6,7,8,9,10])\n",
        "print(reward_matrix)"
      ],
      "metadata": {
        "id": "OUqPgOl0eh2u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4b295f1-e39a-4e14-8144-06d2d0be4679"
      },
      "execution_count": 191,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 0  1  2  3  4  5  6  7  8  9 10]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Take Action Randomly"
      ],
      "metadata": {
        "id": "Af-CAmdfkDQY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Define shoot()\n",
        "\n",
        "def shoot():\n",
        "  return np.random.randint(0,10)\n",
        "\n",
        "action = shoot()\n",
        "print(acton)\n"
      ],
      "metadata": {
        "id": "ibSLCyMyigmK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        },
        "outputId": "1341adab-d877-4476-f36d-e4e7eb2ea1c8"
      },
      "execution_count": 192,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-192-945461ae5f30>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshoot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macton\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'acton' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q matrix\n",
        "\n",
        "**Q-learning** is a reinforcement learning algorithm. Given the current state, it helps to find the best action to be taken by the agent.\n",
        "\n",
        "**Q-matrix** represents reward received after a taking particular action in the current state. Initially, all the elements of the Q-matrix are zeroes.\n"
      ],
      "metadata": {
        "id": "JXKyVT28hHoH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Create Q-matrix\n",
        "q_matrix = np.zeros(11)\n",
        "print(q_matrix)\n"
      ],
      "metadata": {
        "id": "aNYwOV7ogtw1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Take Action"
      ],
      "metadata": {
        "id": "0c95A4SOkGdN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Define take_action\n",
        "\n",
        "def take_action(reward_matrix):\n",
        "\n",
        "     #Call the shoot() function to get the action\n",
        "  action = shoot()\n",
        "  print(\"Action:\", action)\n",
        "     #Get the corresponding reward using Reward matrix\n",
        "  reward = reward_matrix[action]\n",
        "  print(\"Reward: \" , reward)\n",
        "\n",
        "  return action, reward\n",
        "\n",
        "#Call take_action function\n",
        "\n",
        "action_taken = take_action(reward_matrix)"
      ],
      "metadata": {
        "id": "LSBm-8CJ0UfK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Update Q-Matrix"
      ],
      "metadata": {
        "id": "cKy1VkgO4ZhP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Define run_episode() method\n",
        "\n",
        "def run_episode(reward_matrix, shoot_per_game=5):\n",
        "\n",
        "  score = 0\n",
        "\n",
        "  #use for loop to iterate for number of chances\n",
        "  for chance in range(shoot_per_game):\n",
        "        #print shoot number\n",
        "        print(\"Chances remaining :\" , shoot_per_game)\n",
        "        #call take_action method to get the action\n",
        "        score_temp , reward_temp = take_action(reward_matrix)\n",
        "        #increase the score\n",
        "        score += score_temp\n",
        "        # print shoot number ends\n",
        "        print(score)\n",
        "        #Update Q-matrix\n",
        "        q_matrix[score_temp -1] += 1\n",
        "        shoot_per_game = shoot_per_game - 1\n",
        "\n",
        "  #return updated Q-matrix\n",
        "  return q_matrix ,score"
      ],
      "metadata": {
        "id": "_U6NFICkhGMF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Call run_episode method to check the final Q-matrix for one episode\n",
        "\n",
        "q_matrix_updated = run_episode(reward_matrix)\n",
        "print(\"Updated matrix: \",q_matrix_updated)"
      ],
      "metadata": {
        "id": "n5pcUO-5pJ-h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train\n",
        "\n",
        "Create a function that plays  number of games, runs each game for a given number of times, and calculates the mean rewards for each."
      ],
      "metadata": {
        "id": "0dVY734TlZBx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Define train() function\n",
        "\n",
        "def train(episodes):\n",
        "    for episode in range(episodes):\n",
        "      total_reward = 0\n",
        "      print(\"Episode start:\" , episode)\n",
        "      q_matrix , reward = run_episode(reward_matrix)\n",
        "      total_reward += reward\n",
        "      print(reward)\n",
        "    return total_reward\n"
      ],
      "metadata": {
        "id": "smfOXGYZlp7b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Train for 1000 episodes"
      ],
      "metadata": {
        "id": "7WYH_ioykcBC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Run the train() function for 1000 episodes\n",
        "total_rewards = train(1000)\n",
        "print(\"Average: \" , total_rewards/1000)"
      ],
      "metadata": {
        "id": "cbjnp3PSovsE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion:\n",
        "\n",
        "This gives us a good idea about the general performance of the simplest reinforcement learning with **one state multiple action** problem also known as \"**K-Armed Bandit**\" problem.\n",
        "\n",
        "One of the major use cases of this type of problem can be seen in selecting the right advertisement out many to be displayed on the web page. The machine can be taught to pick the best advertisement with the most user clicks!!\n",
        "\n"
      ],
      "metadata": {
        "id": "-jBnwogyuJP0"
      }
    }
  ]
}